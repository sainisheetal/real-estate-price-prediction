{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# To collect the links:\n",
    "import os\n",
    "import seaborn\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as ul\n",
    "import requests as r\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from parsel import Selector \n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "\n",
    "house_links = [] \n",
    "for i in range(1,2):\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    url = ('https://www.immoweb.be/en/search/house/for-rent?countries=BE&page=' + str(i) + '&orderBy=relevance')\n",
    "    driver.get(url)\n",
    "\n",
    "\n",
    "    print(\"Search Criteria :\"  + url)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")   \n",
    "\n",
    "    \n",
    "    rawhouselinks = soup.find_all(\"a\", {\"class\":\"card__title-link\"}) \n",
    "\n",
    "    for item in rawhouselinks:\n",
    "        try:\n",
    "\n",
    "            link = item.get('href')         \n",
    "            house_links.append(link)\n",
    "\n",
    "        except AttributeError:\n",
    "            rawhouselinks != 'data'        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import bs4\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import Safari\n",
    "import pandas as pd\n",
    "import threading\n",
    "import numpy as np\n",
    "dict = { 'ID':[], 'Locality':[],'PricePr': [], 'Tenement building':[],'Venue of the sale':[],\n",
    "      'Bedrooms':[],  \"Living area\":[], \"Kitchen type\":[], \"Furnished\" : [] , 'How many fireplaces?':[], 'Terrace':[], 'Terrace surface':[], \n",
    "       'Garden':[], 'Garden surface':[], 'Surface of the plot':[], 'Number of frontages': [], 'Swimming pool':[], 'Building condition':[]}\n",
    "data_columns = list(dict.keys())\n",
    "\n",
    "with open(\"team1.txt\") as f:\n",
    "    links = f.read().split(\"\\n\")\n",
    "    for var in links:\n",
    "        try:\n",
    "            \n",
    "            thread = threading.Thread(target=drive.get(url = var))\n",
    "            soup = bs4.BeautifulSoup(drive.page_source, \"html.parser\") \n",
    "            table_datas = soup.find_all(\"tr\", class_= \"classified-table__row\")\n",
    "            thread.start()\n",
    "            sleep(3)\n",
    "            headers = []\n",
    "            dict['ID'].append(var.rsplit('/', 1)[1])\n",
    "            headers.append('ID')\n",
    "            dict['Locality'].append(var.rsplit('/')[-3])\n",
    "            headers.append('Locality')\n",
    "            price = soup.find('p', class_='classified__price').find('span', class_='sr-only').string\n",
    "            dict[\"PricePr\"].append(price)\n",
    "            headers.append('PricePr')\n",
    "            for x in table_datas:\n",
    "                try:\n",
    "                    table_data = x.find(\"td\", class_=\"classified-table__data\").text.strip()\n",
    "                    table_data = table_data.replace('square meters', '').replace('mÂ²', '').replace('m', '').strip(\"\\n\").strip(\"\\t\").strip()\n",
    "                except AttributeError:\n",
    "                    table_data = np.NaN\n",
    "                try:\n",
    "                    table_header = x.find(\"th\", class_=\"classified-table__header\").text.strip()\n",
    "                    headers.append(table_header)\n",
    "                except AttributeError:\n",
    "                    table_header = np.NaN\n",
    "                \n",
    "                if table_header == 'Tenement building':\n",
    "                    dict[\"Tenement building\"].append(table_data)\n",
    "                if table_header == 'Venue of the sale':\n",
    "                    dict[\"Venue of the sale\"].append(table_data)\n",
    "                if table_header == 'Living area':\n",
    "                    dict[\"Living area\"].append(table_data)\n",
    "                if table_header == 'Kitchen type':\n",
    "                    dict[\"Kitchen type\"].append(table_data)\n",
    "                if table_header =='Bedrooms':\n",
    "                    dict[\"Bedrooms\"].append(table_data)\n",
    "                if table_header == 'Surface of the plot':\n",
    "                    dict[\"Surface of the plot\"].append(table_data)\n",
    "                if table_header == 'Terrace':\n",
    "                    dict[\"Terrace\"].append(table_data)\n",
    "                if table_header == 'Terrace surface':\n",
    "                    dict[\"Terrace surface\"].append(table_data)\n",
    "                if table_header == 'Garden surface':\n",
    "                    dict[\"Garden surface\"].append(table_data)\n",
    "                if table_header == 'Garden':\n",
    "                    dict[\"Garden\"].append(table_data)\n",
    "                if table_header == 'Furnished':\n",
    "                    dict[\"Furnished\"].append(table_data)\n",
    "                if table_header == 'How many fireplaces?':\n",
    "                    dict[\"How many fireplaces?\"].append(table_data)\n",
    "                if table_header == 'Number of frontages':\n",
    "                    dict[\"Number of frontages\"].append(table_data)\n",
    "                if table_header == 'Swimming pool':\n",
    "                    dict[\"Swimming pool\"].append(table_data)\n",
    "                if table_header == 'Building condition':\n",
    "                    dict[\"Building condition\"].append(table_data)\n",
    "            diff = list(set(data_columns)-set(headers))\n",
    "            for y in diff:\n",
    "                dict[y].append(np.NaN)\n",
    "            sleep(3) \n",
    "            thread.join()\n",
    "        except TimeoutError:\n",
    "            print('This is TimeoutError')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "new_dict = pd.DataFrame(dict)\n",
    "with open('page_2.csv',\"w\") as f:\n",
    "    new_dict.to_csv('page_2.csv', index=False)\n",
    "\n",
    "with open('page_2.csv',\"r\") as f:\n",
    "    df=pd.read_csv('page_2.csv')\n",
    "#code to add new column bv. to fill null values with Brussel\n",
    "#df['Neighbourhood or locality'] = df['Neighbourhood or locality'].fillna('Brussel')\n",
    "print(df)\n",
    "with open('page_2.csv',\"w\") as f:\n",
    "    df.to_csv('page_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_array = []\n",
    "clean_array = []\n",
    "\n",
    "#given different files with links of subcategories\n",
    "with open('links_5.txt',\"r\") as f:\n",
    "    cat_array = f.read().split(\"\\n\")\n",
    "with open('links_6.txt',\"r\") as r:\n",
    "    cat_array1 = r.read().split(\"\\n\")\n",
    "\n",
    "#given the links with all the urls\n",
    "with open('links_10.txt',\"r\") as h:\n",
    "    all_links_array = h.read().split(\"\\n\")\n",
    "\n",
    "#substract the subcategories from all\n",
    "clean_array = list(set(all_links_array)-set(filter(None, cat_array1+cat_array)))\n",
    "\n",
    "#write the output to a new file and use the files with the subcategories apart and add column Sub type\n",
    "with open('links_13.txt',\"w\") as f:\n",
    "    for x in clean_array:\n",
    "        f.write(x+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links_array = []\n",
    "clean_array = []\n",
    "\n",
    "#given different files with links of subcategories\n",
    "with open('links_5.txt',\"r\") as f:\n",
    "    cat_array = f.read().split(\"\\n\")\n",
    "with open('links_6.txt',\"r\") as r:\n",
    "    cat_array1 = r.read().split(\"\\n\")\n",
    "\n",
    "#given the links with all the urls\n",
    "with open('links_10.txt',\"r\") as h:\n",
    "    all_links_array = h.read().split(\"\\n\")\n",
    "\n",
    "#substract the subcategories from all\n",
    "clean_array = list(set(all_links_array)-set(filter(None, cat_array1+cat_array)))\n",
    "\n",
    "#write the output to a new file and use the files with the subcategories apart and add column Sub type\n",
    "with open('links_13.txt',\"w\") as f:\n",
    "    for x in clean_array:\n",
    "        f.write(x+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "308159fa9b6c6619a1f26cdc1093eb362191d23a9b1739232d13ec31d5a998e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
